{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ff207-be89-4b27-a92c-eebe3fb79551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63deea0-e7fc-4766-939d-3894881f1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCHRITT 1: SQLite-Datenbank erstellen ---\n",
    "\n",
    "def create_database(db_path='weihnachten_schnee.db'):\n",
    "    \"\"\"Erstellt die Datenbank mit Star-Schema\"\"\"\n",
    "    print(\"Erstelle Datenbank...\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Dimension: Stationen\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS dim_station (\n",
    "            station_id INTEGER PRIMARY KEY,\n",
    "            station_name TEXT,\n",
    "            bundesland TEXT,\n",
    "            geo_breite REAL,\n",
    "            geo_laenge REAL,\n",
    "            hoehe INTEGER,\n",
    "            von_datum DATE,\n",
    "            bis_datum DATE,\n",
    "            jahre_daten REAL\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Dimension: Datum\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS dim_datum (\n",
    "            datum_id INTEGER PRIMARY KEY,\n",
    "            datum DATE UNIQUE,\n",
    "            jahr INTEGER,\n",
    "            monat INTEGER,\n",
    "            tag INTEGER,\n",
    "            weihnachtstag TEXT  -- '24. Dezember', '25. Dezember', '26. Dezember'\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Fakten: Schneemessungen\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS fakt_schnee (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            station_id INTEGER,\n",
    "            datum_id INTEGER,\n",
    "            schnee_vorhanden INTEGER,  -- 0=nein, 1=ja, NULL=keine Messung\n",
    "            schneehoehe_cm REAL,       -- Original-Wert falls interessant\n",
    "            qualitaet INTEGER,         -- QN_4 vom DWD\n",
    "            FOREIGN KEY (station_id) REFERENCES dim_station(station_id),\n",
    "            FOREIGN KEY (datum_id) REFERENCES dim_datum(datum_id),\n",
    "            UNIQUE(station_id, datum_id)  -- Eine Messung pro Station/Tag\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Indizes für Performance\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_fakt_station ON fakt_schnee(station_id)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_fakt_datum ON fakt_schnee(datum_id)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_datum_jahr ON dim_datum(jahr)')\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"✓ Datenbank erstellt\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a083abc2-762c-4305-9b03-e3c880067495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCHRITT 2: Stationen laden ---\n",
    "\n",
    "def load_stations(conn, stations_csv='hexagons_mit_wetterstationen.csv'):\n",
    "    \"\"\"Lädt Stationsdaten aus deinem vorherigen Output\"\"\"\n",
    "    print(\"\\nLade Stationen in Datenbank...\")\n",
    "    \n",
    "    from io import StringIO\n",
    "    \n",
    "    KLIMA_URL = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/\"\n",
    "    \n",
    "    response = requests.get(KLIMA_URL)\n",
    "    import re\n",
    "    match = re.search(r'href=\"([^\"]*Beschreibung_Stationen\\.txt)\"', response.text)\n",
    "    \n",
    "    if match:\n",
    "        full_url = KLIMA_URL + match.group(1)\n",
    "        stations_response = requests.get(full_url)\n",
    "        stations_response.encoding = 'latin1'\n",
    "        text = stations_response.text\n",
    "        \n",
    "        # Parse Stationsliste\n",
    "        lines = text.split('\\n')\n",
    "        header_line = None\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'Stations_id' in line and 'von_datum' in line:\n",
    "                header_line = i\n",
    "                data_start = i + 2\n",
    "                break\n",
    "        \n",
    "        data = []\n",
    "        for line in lines[data_start:]:\n",
    "            if line.strip() and not line.startswith('-'):\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 8:\n",
    "                    data.append(parts[:8])\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=[\n",
    "            'station_id', 'von_datum', 'bis_datum',\n",
    "            'hoehe', 'geo_breite', 'geo_laenge',\n",
    "            'station_name', 'bundesland'\n",
    "        ])\n",
    "        \n",
    "        df['station_id'] = df['station_id'].astype(int)\n",
    "        df['geo_breite'] = df['geo_breite'].astype(float)\n",
    "        df['geo_laenge'] = df['geo_laenge'].astype(float)\n",
    "        df['hoehe'] = df['hoehe'].astype(int)\n",
    "        df['von_datum'] = pd.to_datetime(df['von_datum'], format='%Y%m%d')\n",
    "        df['bis_datum'] = pd.to_datetime(df['bis_datum'], format='%Y%m%d')\n",
    "        df['jahre_daten'] = ((df['bis_datum'] - df['von_datum']).dt.days / 365.25).round(1)\n",
    "        \n",
    "        # In Datenbank schreiben\n",
    "        df.to_sql('dim_station', conn, if_exists='replace', index=False)\n",
    "        \n",
    "        print(f\"✓ {len(df)} Stationen geladen\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f47c13-e629-4118-855c-58a1f488135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCHRITT 3: Datumsdimension füllen ---\n",
    "\n",
    "def create_date_dimension(conn, start_year=1880, end_year=2024):\n",
    "    \"\"\"Erstellt Datumsdimension für alle Weihnachtstage\"\"\"\n",
    "    print(\"\\nErstelle Datumsdimension...\")\n",
    "    \n",
    "    dates = []\n",
    "    datum_id = 1\n",
    "    \n",
    "    for jahr in range(start_year, end_year + 1):\n",
    "        for tag, label in [(24, '24. Dezember'), (25, '25. Dezember'), (26, '26. Dezember')]:\n",
    "            datum = datetime(jahr, 12, tag)\n",
    "            dates.append({\n",
    "                'datum_id': datum_id,\n",
    "                'datum': datum.strftime('%Y-%m-%d'),\n",
    "                'jahr': jahr,\n",
    "                'monat': 12,\n",
    "                'tag': tag,\n",
    "                'weihnachtstag': label\n",
    "            })\n",
    "            datum_id += 1\n",
    "    \n",
    "    df_dates = pd.DataFrame(dates)\n",
    "    df_dates.to_sql('dim_datum', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    print(f\"✓ {len(df_dates)} Datumseinträge erstellt ({start_year}-{end_year})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc0366-d4dd-4586-b33d-104d0082a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCHRITT 4: Schneedaten vom DWD laden ---\n",
    "\n",
    "def download_snow_data_for_station(station_id, base_url):\n",
    "    \"\"\"Lädt Schneehöhen-Daten für eine Station\"\"\"\n",
    "    try:\n",
    "        # Suche nach der ZIP-Datei für diese Station\n",
    "        response = requests.get(base_url, timeout=10)\n",
    "        \n",
    "        import re\n",
    "        pattern = f'href=\"(tageswerte_KL_{station_id:05d}_[0-9]+_[0-9]+_hist\\\\.zip)\"'\n",
    "        match = re.search(pattern, response.text)\n",
    "        \n",
    "        if not match:\n",
    "            return None\n",
    "        \n",
    "        zip_url = base_url + match.group(1)\n",
    "        \n",
    "        # Download ZIP\n",
    "        zip_response = requests.get(zip_url, timeout=30)\n",
    "        \n",
    "        # Entpacke und lese Produktdatei\n",
    "        with zipfile.ZipFile(BytesIO(zip_response.content)) as z:\n",
    "            # Finde die produkt*.txt Datei\n",
    "            txt_files = [f for f in z.namelist() if f.startswith('produkt_klima_tag')]\n",
    "            \n",
    "            if not txt_files:\n",
    "                return None\n",
    "            \n",
    "            with z.open(txt_files[0]) as f:\n",
    "                df = pd.read_csv(f, sep=';', skipinitialspace=True)\n",
    "                \n",
    "                # Bereinige Spaltennamen (Leerzeichen entfernen)\n",
    "                df.columns = df.columns.str.strip()\n",
    "                \n",
    "                return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Fehler bei Station {station_id}: {str(e)[:50]}\")\n",
    "        return None\n",
    "\n",
    "def load_snow_data(conn, df_stations, limit=None):\n",
    "    \"\"\"Lädt Schneehöhen für alle Stationen (nur 24.-26. Dez)\"\"\"\n",
    "    print(\"\\nLade Schneehöhen-Daten vom DWD...\")\n",
    "    \n",
    "    base_url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/kl/historical/\"\n",
    "    \n",
    "    # Datum-Lookup erstellen\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT datum_id, datum FROM dim_datum')\n",
    "    datum_lookup = {row[1]: row[0] for row in cursor.fetchall()}\n",
    "    \n",
    "    stations_to_process = df_stations.head(limit) if limit else df_stations\n",
    "    \n",
    "    total_records = 0\n",
    "    failed_stations = []\n",
    "    \n",
    "    for idx, station in stations_to_process.iterrows():\n",
    "        station_id = station['station_id']\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Fortschritt: {idx + 1}/{len(stations_to_process)} Stationen...\")\n",
    "        \n",
    "        df_data = download_snow_data_for_station(station_id, base_url)\n",
    "        \n",
    "        if df_data is None:\n",
    "            failed_stations.append(station_id)\n",
    "            continue\n",
    "        \n",
    "        # Konvertiere Datum\n",
    "        df_data['MESS_DATUM'] = pd.to_datetime(df_data['MESS_DATUM'].astype(str), format='%Y%m%d')\n",
    "        \n",
    "        # Filtere nur 24.-26. Dezember\n",
    "        df_christmas = df_data[\n",
    "            (df_data['MESS_DATUM'].dt.month == 12) & \n",
    "            (df_data['MESS_DATUM'].dt.day.isin([24, 25, 26]))\n",
    "        ].copy()\n",
    "        \n",
    "        if len(df_christmas) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Verarbeite Schneehöhe\n",
    "        df_christmas['datum_str'] = df_christmas['MESS_DATUM'].dt.strftime('%Y-%m-%d')\n",
    "        df_christmas['datum_id'] = df_christmas['datum_str'].map(datum_lookup)\n",
    "        \n",
    "        # Schneehöhe verarbeiten (-999 = keine Messung)\n",
    "        df_christmas['SHK_TAG'] = pd.to_numeric(df_christmas['SHK_TAG'], errors='coerce')\n",
    "        df_christmas['schnee_vorhanden'] = df_christmas['SHK_TAG'].apply(\n",
    "            lambda x: 1 if x > 0 else (0 if x == 0 else None)\n",
    "        )\n",
    "        \n",
    "        # Bereite Fakten vor\n",
    "        facts = df_christmas[[\n",
    "            'STATIONS_ID', 'datum_id', 'schnee_vorhanden', 'SHK_TAG', 'QN_4'\n",
    "        ]].copy()\n",
    "        \n",
    "        facts.columns = ['station_id', 'datum_id', 'schnee_vorhanden', 'schneehoehe_cm', 'qualitaet']\n",
    "        facts = facts.dropna(subset=['datum_id'])\n",
    "        \n",
    "        # In Datenbank schreiben (mit INSERT OR REPLACE für Duplikate)\n",
    "        cursor = conn.cursor()\n",
    "        for _, row in facts.iterrows():\n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO fakt_schnee \n",
    "                (station_id, datum_id, schnee_vorhanden, schneehoehe_cm, qualitaet)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                int(row['station_id']), \n",
    "                int(row['datum_id']), \n",
    "                int(row['schnee_vorhanden']) if pd.notna(row['schnee_vorhanden']) else None,\n",
    "                float(row['schneehoehe_cm']) if pd.notna(row['schneehoehe_cm']) else None,\n",
    "                int(row['qualitaet']) if pd.notna(row['qualitaet']) else None\n",
    "            ))\n",
    "        total_records += len(facts)\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"\\n✓ {total_records} Schneemessungen geladen\")\n",
    "    print(f\"✓ {len(failed_stations)} Stationen ohne Daten\")\n",
    "    \n",
    "    return failed_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0673c04-7edd-4b3a-930e-8cd7e03f84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SCHRITT 5: Hauptprogramm ---\n",
    "\n",
    "def main():\n",
    "    # Datenbank erstellen\n",
    "    conn = create_database('weihnachten_schnee.db')\n",
    "    \n",
    "    # Stationen laden\n",
    "    df_stations = load_stations(conn)\n",
    "    \n",
    "    # Datumsdimension erstellen\n",
    "    create_date_dimension(conn, start_year=1880, end_year=2024)\n",
    "    \n",
    "    failed = load_snow_data(conn, df_stations, limit=None)  # Erst mal nur 10 zum Testen\n",
    "    \n",
    "    # Statistiken\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATENBANK-STATISTIKEN:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(*) FROM dim_station')\n",
    "    print(f\"Stationen: {cursor.fetchone()[0]}\")\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(*) FROM dim_datum')\n",
    "    print(f\"Datumseinträge: {cursor.fetchone()[0]}\")\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(*) FROM fakt_schnee')\n",
    "    print(f\"Schneemessungen: {cursor.fetchone()[0]}\")\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT \n",
    "            SUM(CASE WHEN schnee_vorhanden = 1 THEN 1 ELSE 0 END) as mit_schnee,\n",
    "            SUM(CASE WHEN schnee_vorhanden = 0 THEN 1 ELSE 0 END) as ohne_schnee,\n",
    "            SUM(CASE WHEN schnee_vorhanden IS NULL THEN 1 ELSE 0 END) as keine_messung\n",
    "        FROM fakt_schnee\n",
    "    ''')\n",
    "    \n",
    "    stats = cursor.fetchone()\n",
    "    print(f\"\\nMit Schnee: {stats[0]}\")\n",
    "    print(f\"Ohne Schnee: {stats[1]}\")\n",
    "    print(f\"Keine Messung: {stats[2]}\")\n",
    "    \n",
    "    \n",
    "    df_example = pd.read_sql_query(query, conn)\n",
    "    print(df_example.to_string(index=False))\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"\\n✓ Fertig! Datenbank: weihnachten_schnee.db\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2306df0-0bdb-43d4-a102-98c288ca37fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
